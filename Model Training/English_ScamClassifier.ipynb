{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F48vjuZ-PMKV",
        "outputId": "191dfce5-de4d-4fe3-d7c8-968bbee400e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting sparknlp\n",
            "  Downloading sparknlp-1.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Collecting spark-nlp (from sparknlp)\n",
            "  Downloading spark_nlp-5.5.2-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spark_nlp-5.5.2-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.3/636.3 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp, xxhash, sparknlp, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 spark-nlp-5.5.2 sparknlp-1.0.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install pyspark nltk datasets sparknlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6AX8Vu7PNF7",
        "outputId": "4fa0085a-3a9c-445d-8b77-60215c4fbd53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import sparknlp\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "FlChvoydPO3u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from pyspark.ml.feature import StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import lower, regexp_replace, udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datasets import load_dataset\n",
        "from nltk.stem import PorterStemmer\n",
        "from sparknlp.base import DocumentAssembler, TokenAssembler, Finisher\n",
        "from sparknlp.annotator import Tokenizer, LemmatizerModel\n",
        "from sparknlp.annotator import Normalizer\n",
        "from pyspark.sql.functions import rand\n"
      ],
      "metadata": {
        "id": "PX7C_Di6PQVm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {'train': 'scam-dialogue_train.csv', 'test': 'scam-dialogue_test.csv'}\n",
        "\n",
        "# Load the dataset using Hugging Face's datasets library\n",
        "dataset = load_dataset(\"BothBosu/scam-dialogue\", split=\"train\")\n",
        "\n",
        "# Convert the Hugging Face dataset to a Spark DataFrame\n",
        "train_df = spark.createDataFrame(dataset.to_pandas())\n",
        "\n",
        "# Display first few rows\n",
        "train_df.orderBy(rand()).limit(10).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFRGbdbkPaCL",
        "outputId": "103299ef-92f0-4d74-d7c5-3d79a38c55e8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+-----+\n",
            "|            dialogue|         type|label|\n",
            "+--------------------+-------------+-----+\n",
            "|caller: Hi, is Jo...|        wrong|    0|\n",
            "|caller: Hello, my...|       reward|    1|\n",
            "|caller: Hello, my...|       reward|    1|\n",
            "|caller: Hello, my...|    insurance|    0|\n",
            "|caller: Hello, my...|       reward|    1|\n",
            "|caller: Hello, my...|    insurance|    0|\n",
            "|caller: Hi, I'm c...|     delivery|    0|\n",
            "|caller: Hi, my na...|    insurance|    0|\n",
            "|caller: Hello, co...|       reward|    1|\n",
            "|caller: Hi, my na...|telemarketing|    0|\n",
            "+--------------------+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess training data\n",
        "# Text preprocessing\n",
        "train_df = train_df.withColumn(\"dialogue\", lower(train_df.dialogue))\n",
        "train_df = train_df.withColumn(\"dialogue\", regexp_replace(train_df.dialogue, \"[^a-zA-Z\\\\s]\", \"\"))\n"
      ],
      "metadata": {
        "id": "aV5T8unAPaUz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing pipeline\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"dialogue\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "spark_nlp_tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\") \\\n",
        "    .setLowercase(True) \\\n",
        "    .setCleanupPatterns([\"[^a-zA-Z\\\\s]\"])\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\") \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"lemmas\")\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"lemmas\"]) \\\n",
        "    .setOutputCols([\"finished_lemmas\"]) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(False)\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"finished_lemmas\", outputCol=\"filtered_words\")\n",
        "count_vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"label\",\n",
        "                          featuresCol=\"features\",\n",
        "                          numTrees=50,\n",
        "                          seed=42)"
      ],
      "metadata": {
        "id": "2Fx8aqxeQYMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2104bbe-7e4f-41dc-9b8e-01ed2c4443e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    spark_nlp_tokenizer,\n",
        "    normalizer,\n",
        "    lemmatizer,\n",
        "    finisher,\n",
        "    stopwords_remover,\n",
        "    count_vectorizer,\n",
        "    idf,\n",
        "    rf\n",
        "])"
      ],
      "metadata": {
        "id": "0V56ekZmkayJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.fit(train_df)"
      ],
      "metadata": {
        "id": "X5cJdykDmV69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"random_forest_model\")"
      ],
      "metadata": {
        "id": "DbqCqtajYkDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess test data\n",
        "test_dataset = load_dataset(\"BothBosu/scam-dialogue\", split=\"test\")\n",
        "# Convert the Hugging Face dataset to a Spark DataFrame\n",
        "test_df = spark.createDataFrame(test_dataset.to_pandas())"
      ],
      "metadata": {
        "id": "ZYWTRfHVQ_gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(test_df)"
      ],
      "metadata": {
        "id": "82a4ILyNRGYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Shuffle and display the first 5 rows\n",
        "predictions.orderBy(rand()).select(\"label\", \"prediction\").show(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b1s4ZZ4hmEq",
        "outputId": "a7882949-79a9-4962-87a2-29736ab56940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    1|       1.0|\n",
            "|    0|       0.0|\n",
            "|    1|       1.0|\n",
            "|    0|       0.0|\n",
            "|    1|       1.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    1|       1.0|\n",
            "|    1|       1.0|\n",
            "|    0|       0.0|\n",
            "|    1|       1.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    1|       1.0|\n",
            "|    1|       1.0|\n",
            "+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")"
      ],
      "metadata": {
        "id": "n9Oet8mJRcn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/random_forest_model.zip /content/random_forest_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uq0cCf9c660",
        "outputId": "c1044afa-90fd-45de-d909-016330fca90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/random_forest_model/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/metadata/part-00000 (deflated 37%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/fields/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/fields/slangDict/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/fields/slangDict/part-00000 (deflated 27%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/fields/slangDict/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/fields/slangDict/.part-00001.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/fields/slangDict/part-00001 (deflated 27%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/fields/slangDict/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/2_NORMALIZER_2a06bc737c29/fields/slangDict/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/4_Finisher_a8240e92534e/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/4_Finisher_a8240e92534e/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/4_Finisher_a8240e92534e/metadata/part-00000 (deflated 41%)\n",
            "  adding: content/random_forest_model/stages/4_Finisher_a8240e92534e/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/4_Finisher_a8240e92534e/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/4_Finisher_a8240e92534e/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/metadata/part-00000 (deflated 20%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/fields/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/fields/lemmaDict/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/fields/lemmaDict/part-00000 (deflated 75%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/fields/lemmaDict/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/fields/lemmaDict/.part-00001.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/fields/lemmaDict/part-00001 (deflated 75%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/fields/lemmaDict/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/3_LEMMATIZER_c62ad8f355f9/fields/lemmaDict/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/data/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/data/.part-00000-c2746db2-40ec-405d-b4cc-faaaf55d8bc3-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/data/part-00000-c2746db2-40ec-405d-b4cc-faaaf55d8bc3-c000.snappy.parquet (deflated 15%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/data/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/metadata/part-00000 (deflated 35%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/6_CountVectorizer_4e4cc5a3877b/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/metadata/part-00000 (deflated 44%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/fields/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/fields/rules/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/fields/rules/part-00000 (deflated 27%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/fields/rules/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/fields/rules/.part-00001.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/fields/rules/part-00001 (deflated 62%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/fields/rules/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/1_REGEX_TOKENIZER_b999f8075cf0/fields/rules/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/data/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/data/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/data/.part-00000-3e6cb9c7-e25f-413e-8d90-f9c7f6af213c-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/data/part-00000-3e6cb9c7-e25f-413e-8d90-f9c7f6af213c-c000.snappy.parquet (deflated 42%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/metadata/part-00000 (deflated 32%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/7_IDF_385a646d7589/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/treesMetadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/treesMetadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/treesMetadata/.part-00000-84ab037f-afcf-4009-b2e0-8e8f488ed4c2-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/treesMetadata/part-00000-84ab037f-afcf-4009-b2e0-8e8f488ed4c2-c000.snappy.parquet (deflated 32%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/treesMetadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/data/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/data/part-00000-ff7b467e-87f6-4f94-b889-7efe45b49d52-c000.snappy.parquet (deflated 18%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/data/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/data/.part-00000-ff7b467e-87f6-4f94-b889-7efe45b49d52-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/metadata/part-00000 (deflated 46%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/8_RandomForestClassifier_2f5caea0fdf8/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/0_DocumentAssembler_8efb2c47903f/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/0_DocumentAssembler_8efb2c47903f/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/0_DocumentAssembler_8efb2c47903f/metadata/part-00000 (deflated 33%)\n",
            "  adding: content/random_forest_model/stages/0_DocumentAssembler_8efb2c47903f/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/0_DocumentAssembler_8efb2c47903f/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/0_DocumentAssembler_8efb2c47903f/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/5_StopWordsRemover_4ed1de7f3339/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/5_StopWordsRemover_4ed1de7f3339/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/stages/5_StopWordsRemover_4ed1de7f3339/metadata/part-00000 (deflated 58%)\n",
            "  adding: content/random_forest_model/stages/5_StopWordsRemover_4ed1de7f3339/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/stages/5_StopWordsRemover_4ed1de7f3339/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/stages/5_StopWordsRemover_4ed1de7f3339/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/random_forest_model/metadata/ (stored 0%)\n",
            "  adding: content/random_forest_model/metadata/part-00000 (deflated 26%)\n",
            "  adding: content/random_forest_model/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/random_forest_model/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/random_forest_model/metadata/.part-00000.crc (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Metrics\n",
        "def compute_classification_metrics(predictions):\n",
        "    evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "    evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "    evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "    precision = evaluator_precision.evaluate(predictions)\n",
        "    recall = evaluator_recall.evaluate(predictions)\n",
        "    f1 = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "    confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
        "\n",
        "    return precision, recall, f1, confusion_matrix"
      ],
      "metadata": {
        "id": "Mw-Tk9uwZGsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "\n",
        "precision, recall, f1, confusion_matrix = compute_classification_metrics(predictions)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "confusion_matrix.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BASHkP83ReQ1",
        "outputId": "e303cc73-4229-451c-981b-3fadffba64b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "\n",
            "Confusion Matrix:\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       0.0|  160|\n",
            "|    1|       1.0|  160|\n",
            "+-----+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict on new text\n",
        "def predict_scam_or_not(text):\n",
        "    # Create a single row DataFrame\n",
        "    data = [(text,)]\n",
        "    df = spark.createDataFrame(data, [\"dialogue\"])\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.transform(df)\n",
        "    result = prediction.select(\"prediction\").collect()[0][0]\n",
        "\n",
        "    return \"Scam\" if result == 1 else \"Non-Scam\""
      ],
      "metadata": {
        "id": "mXpbm9UJRgd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample transcripts\n",
        "transcripts = [\n",
        "    # Scam transcript\n",
        "    \"\"\"\n",
        "    caller: Hello, this is Professor Armin from the University of Calgary. We've detected suspicious activity related to your course 550 project.\n",
        "    receiver: Suspicious activity? What do you mean?\n",
        "    caller: It appears that unauthorized access has been detected. We need your student ID and password to secure your account immediately.\n",
        "    receiver: That sounds alarming. Shouldn't the IT department handle this?\n",
        "    caller: I'm authorized to assist. Time is critical to prevent data loss. Please provide your credentials now.\n",
        "    \"\"\",\n",
        "    # Scam transcript\n",
        "    \"\"\"\n",
        "    caller: Hello, this is Dr. Thompson from the University of Calgary IT Security Division. We've detected suspicious activity.\n",
        "    receiver: Suspicious activity?\n",
        "    caller: Yes, we've detected multiple unauthorized login attempts. We need your student ID and password to secure your account immediately.\n",
        "    receiver: This is concerning. Should I contact my supervisor about this?\n",
        "    caller: There's no time for that. I need your university login credentials and student ID immediately to block these attacks and secure your research files.\n",
        "    \"\"\",\n",
        "    # Non-Scam transcript\n",
        "    \"\"\"\n",
        "    receiver: Hi Professor Armin, I wanted to discuss the upcoming demo for course 550.\n",
        "    professor: Of course! I'm excited to see what you and your team have developed.\n",
        "    receiver: Students have been working hard, and we're ready to present to you, the TAs.\n",
        "    professor: Excellent. Let's schedule the presentation in Calgary next week.\n",
        "    \"\"\",\n",
        "    # Non-Scam transcript\n",
        "    \"\"\"\n",
        "    receiver: Hello Professor Armin, we're ready to showcase our course 550 project.\n",
        "    professor: That's wonderful! I'm looking forward to your demo.\n",
        "    receiver: We'll be presenting to you, the TAs  in Calgary.\n",
        "    professor: Sounds great. Make sure to prepare thoroughly.\n",
        "    \"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "zgHRGkedRj8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for idx, transcript in enumerate(transcripts):\n",
        "    result = predict_scam_or_not(transcript)\n",
        "    print(f\"Transcript {idx+1} Prediction: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzfeRua9Rq4n",
        "outputId": "f5383888-56a5-4622-a7d6-8bb1134bd71d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript 1 Prediction: Scam\n",
            "Transcript 2 Prediction: Scam\n",
            "Transcript 3 Prediction: Non-Scam\n",
            "Transcript 4 Prediction: Non-Scam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Hugging Face dataset to a Spark DataFrame\n",
        "test_df_2 = spark.createDataFrame(test_dataset.to_pandas())\n",
        "test_df_2 = test_df_2.orderBy(rand())\n",
        "# Collect the 'dialogue' column as a list of arrays\n",
        "dialogue_array = test_df_2.rdd.map(lambda row: (row['dialogue'], row['label'])).collect()\n",
        "\n",
        "for idx, row in enumerate(dialogue_array[:30]):\n",
        "    result = predict_scam_or_not(row[0])\n",
        "    print(f\"Transcript {idx+1} Prediction: {result}, Actual: {row[1]}\")"
      ],
      "metadata": {
        "id": "efWT-wdhRwpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7tSChuUylMzc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}